import os
import requests
import json
import time
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import hashlib
import sqlite3
from datetime import datetime
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒè®Šæ•¸é…ç½®
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN")
LINE_GROUP_ID = os.environ.get("LINE_GROUP_ID")

# é©—è­‰ç’°å¢ƒè®Šæ•¸
if not DEEPSEEK_API_KEY:
    logger.error("æœªè¨­ç½® DEEPSEEK_API_KEY ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® DEEPSEEK_API_KEY ç’°å¢ƒè®Šæ•¸")
    
if not LINE_CHANNEL_ACCESS_TOKEN:
    logger.error("æœªè¨­ç½® LINE_CHANNEL_ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® LINE_CHANNEL_ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸")
    
if not LINE_GROUP_ID:
    logger.error("æœªè¨­ç½® LINE_GROUP_ID ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® LINE_GROUP_ID ç’°å¢ƒè®Šæ•¸")

# Truth Social URL
TRUTH_URL = "https://truthsocial.com/@realDonaldTrump"

# è¨­ç½®æ—¥èªŒé ‚éƒ¨
def log_startup():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info("=" * 50)
    logger.info(f"è…³æœ¬å•Ÿå‹•æ™‚é–“: {current_time}")
    logger.info("=" * 50)

# è…³æœ¬çµæŸæ™‚çš„æ—¥èªŒ
def log_shutdown():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info("=" * 50)
    logger.info(f"è…³æœ¬çµæŸæ™‚é–“: {current_time}")
    logger.info("=" * 50)

# åˆå§‹åŒ–æ•¸æ“šåº«
def init_db():
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS posts (
        post_id TEXT PRIMARY KEY,
        content TEXT,
        created_at TIMESTAMP
    )
    ''')
    conn.commit()
    conn.close()
    logger.info("æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")

# æª¢æŸ¥è²¼æ–‡æ˜¯å¦å·²å­˜åœ¨
def is_post_exists(post_id):
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM posts WHERE post_id = ?", (post_id,))
    exists = cursor.fetchone() is not None
    conn.close()
    return exists

# ä¿å­˜æ–°è²¼æ–‡
def save_post(post_id, content):
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO posts (post_id, content, created_at) VALUES (?, ?, ?)",
        (post_id, content, datetime.now())
    )
    conn.commit()
    conn.close()
    logger.info(f"ä¿å­˜è²¼æ–‡ ID: {post_id}")

# é…ç½® Selenium
def setup_selenium():
    logger.info("è®¾ç½® Selenium")
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    # ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„ Chromium
    chrome_bin = os.environ.get("CHROME_BIN", "/usr/bin/chromium")
    chrome_options.binary_location = chrome_bin
    
    # ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„ chromedriver
    try:
        driver = webdriver.Chrome(options=chrome_options)
        logger.info("æˆåŠŸä½¿ç”¨ç³»ç»Ÿ chromedriver å»ºç«‹ driver")
        return driver
    except Exception as e:
        logger.error(f"å»ºç«‹ driver å¤±è´¥: {e}")
        raise
        
# çˆ¬å– Truth Social è²¼æ–‡
def scrape_truth_social():
    logger.info("é–‹å§‹çˆ¬å– Truth Social")
    
    driver = None
    
    try:
        driver = setup_selenium()
        driver.get(TRUTH_URL)
        logger.info("å·²è¨ªå• Truth Social é é¢")
        
        # ç­‰å¾…é é¢åŠ è¼‰
        WebDriverWait(driver, 20).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "article.status-card, div.status-wrapper"))
        )
        logger.info("é é¢å…ƒç´ å·²åŠ è¼‰")
        
        # ç¢ºä¿é é¢å®Œå…¨åŠ è¼‰
        time.sleep(5)
        
        page_source = driver.page_source
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # å°‹æ‰¾æœ€æ–°çš„è²¼æ–‡
        posts = soup.select('article.status-card, div.status-wrapper')
        
        if not posts:
            logger.warning("æ²’æœ‰æ‰¾åˆ°è²¼æ–‡")
            return None
        
        latest_post = posts[0]
        logger.info("æ‰¾åˆ°æœ€æ–°è²¼æ–‡")
        
        # æå–è²¼æ–‡å…§å®¹
        content_element = latest_post.select_one('div.status-content, div.status-body')
        if not content_element:
            logger.warning("æ‰¾ä¸åˆ°è²¼æ–‡å…§å®¹å…ƒç´ ")
            return None
            
        content = content_element.text.strip()
        
        # ç”Ÿæˆè²¼æ–‡ ID
        post_id = hashlib.md5(content.encode()).hexdigest()
        
        # æª¢æŸ¥æ˜¯å¦æœ‰åª’é«”
        media = latest_post.select('div.media-gallery img, div.media-gallery video, div.media-attachment img, div.media-attachment video')
        media_urls = []
        
        for item in media:
            src = item.get('src') or item.get('data-src')
            if src:
                if not src.startswith(('http://', 'https://')):
                    src = f"https://truthsocial.com{src}" if src.startswith('/') else f"https://truthsocial.com/{src}"
                media_urls.append(src)
        
        logger.info(f"è²¼æ–‡ ID: {post_id}, åª’é«”æ•¸é‡: {len(media_urls)}")
        return {
            'id': post_id,
            'content': content,
            'media_urls': media_urls
        }
        
    except Exception as e:
        logger.error(f"çˆ¬å–å¤±æ•—: {e}")
        return None
    
    finally:
        if driver:
            driver.quit()
            logger.info("Selenium driver å·²é—œé–‰")

# ä½¿ç”¨ DeepSeek API ç¿»è­¯å…§å®¹
def translate_with_deepseek(text):
    logger.info("ä½¿ç”¨ DeepSeek API ç¿»è­¯")
    
    try:
        # ä½¿ç”¨ OpenAI SDK çš„æ–¹å¼ä¾†å‘¼å« DeepSeek API
        from openai import OpenAI
        
        client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ç¿»è­¯ï¼Œè«‹å°‡ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è­¯æˆä¸­æ–‡ã€‚ä¿æŒåŸæ„ï¼Œä½¿èªè¨€æµæš¢è‡ªç„¶ã€‚"},
                {"role": "user", "content": text}
            ],
            temperature=0.3,
            stream=False
        )
        
        translated_text = response.choices[0].message.content
        logger.info("ç¿»è­¯å®Œæˆ")
        return translated_text
        
    except Exception as e:
        logger.error(f"ç¿»è­¯å¤±æ•—: {e}")
        return f"[ç¿»è­¯éŒ¯èª¤] åŸæ–‡: {text}"

# å…§å®¹åˆ†æï¼ˆåˆ¤æ–·æ˜¯æ–‡å­—é‚„æ˜¯è¦–é »ï¼‰
def analyze_content(post):
    if not post:
        return None
        
    # æª¢æŸ¥æ˜¯å¦åŒ…å«è¦–é »
    is_video = any(url.endswith(('.mp4', '.avi', '.mov', '.webm')) for url in post.get('media_urls', []))
    
    # ç¿»è­¯æ–‡æœ¬å…§å®¹
    translated_content = translate_with_deepseek(post['content'])
    
    result = {
        'id': post['id'],
        'original_content': post['content'],
        'translated_content': translated_content,
        'media_urls': post['media_urls'],
        'is_video': is_video
    }
    
    content_type = "å½±ç‰‡" if is_video else "æ–‡å­—"
    logger.info(f"å…§å®¹é¡å‹: {content_type}")
    
    return result

# ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„
def send_to_line_group(message):
    logger.info(f"ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„: {message[:50]}...")
    
    if not LINE_CHANNEL_ACCESS_TOKEN:
        error = "ERROR: LINE_CHANNEL_ACCESS_TOKEN æœªè¨­ç½®"
        logger.error(error)
        return False
        
    if not LINE_GROUP_ID:
        error = "ERROR: LINE_GROUP_ID æœªè¨­ç½®"
        logger.error(error)
        return False
    
    url = 'https://api.line.me/v2/bot/message/push'
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {LINE_CHANNEL_ACCESS_TOKEN}'
    }
    
    data = {
        'to': LINE_GROUP_ID,
        'messages': [
            {
                'type': 'text',
                'text': message
            }
        ]
    }
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        
        # è¼¸å‡ºè©³ç´°å›æ‡‰
        logger.info(f"LINE API éŸ¿æ‡‰ç‹€æ…‹ç¢¼: {response.status_code}")
        logger.info(f"LINE API éŸ¿æ‡‰å…§å®¹: {response.text}")
        
        response.raise_for_status()
        logger.info("LINE æ¶ˆæ¯ç™¼é€æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"LINE æ¶ˆæ¯ç™¼é€å¤±æ•—: {e}")
        return False

# ä¸»æµç¨‹
def main():
    try:
        log_startup()
        
        # ç¬¬ä¸€æ¬¡å•Ÿå‹•æ™‚ç™¼é€é€šçŸ¥ï¼Œç”¨æ–¼ç¢ºèªæ©Ÿå™¨äººæ­£å¸¸å·¥ä½œ
        first_run_file = "first_run_completed.txt"
        first_run = not os.path.exists(first_run_file)
        
        if first_run:
            send_to_line_group("ğŸ¤– Trump ç›£æ§æ©Ÿå™¨äººé¦–æ¬¡å•Ÿå‹•ï¼Œæ­£åœ¨æª¢æŸ¥ Truth Social...")
        
        # åˆå§‹åŒ–æ•¸æ“šåº«
        init_db()
        
        # çˆ¬å–æœ€æ–°è²¼æ–‡
        latest_post = scrape_truth_social()
        
        if not latest_post:
            if first_run:
                send_to_line_group("ğŸ” é¦–æ¬¡çˆ¬å–æ²’æœ‰æ‰¾åˆ°ä»»ä½•è²¼æ–‡ï¼Œå¯èƒ½æ˜¯ç¶²é çµæ§‹è®ŠåŒ–æˆ–è€…çˆ¬èŸ²å•é¡Œã€‚")
            return
            
        # åªåœ¨é¦–æ¬¡é‹è¡Œæ™‚ç™¼é€çˆ¬å–çµæœé€šçŸ¥
        if first_run:
            post_info = f"âœ… é¦–æ¬¡çˆ¬å–æˆåŠŸï¼æ‰¾åˆ°è²¼æ–‡ï¼\n\nID: {latest_post['id']}\n\nå…§å®¹: {latest_post['content'][:100]}...\n\nåª’é«”æ•¸é‡: {len(latest_post['media_urls'])}"
            send_to_line_group(post_info)
            # æ¨™è¨˜é¦–æ¬¡é‹è¡Œå·²å®Œæˆ
            with open(first_run_file, "w") as f:
                f.write("completed")
        
        # æª¢æŸ¥è²¼æ–‡æ˜¯å¦å·²å­˜åœ¨
        if is_post_exists(latest_post['id']):
            logger.info(f"è²¼æ–‡ {latest_post['id']} å·²å­˜åœ¨ï¼Œè·³éè™•ç†")
            return  # éœé»˜è·³éï¼Œä¸ç™¼é€é€šçŸ¥
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå½±ç‰‡è²¼æ–‡
        is_video = any(url.endswith(('.mp4', '.avi', '.mov', '.webm')) for url in latest_post.get('media_urls', []))
        
        if is_video:
            # éœé»˜ç•¥éå½±ç‰‡è²¼æ–‡ï¼Œä½†ä»ç„¶ä¿å­˜åˆ°æ•¸æ“šåº«
            logger.info("æª¢æ¸¬åˆ°å½±ç‰‡è²¼æ–‡ï¼Œç•¥éè™•ç†")
            save_post(latest_post['id'], latest_post['content'])
            return
            
        # åˆ†æä¸¦ç¿»è­¯å…§å®¹ï¼ˆä¸ç™¼é€é€²åº¦é€šçŸ¥ï¼‰
        logger.info("é–‹å§‹åˆ†æä¸¦ç¿»è­¯å…§å®¹")
        processed_content = analyze_content(latest_post)
        
        if not processed_content:
            logger.error("å…§å®¹è™•ç†å¤±æ•—")
            return  # è™•ç†å¤±æ•—ï¼Œéœé»˜è·³é
            
        # æ§‹å»º LINE æ¶ˆæ¯
        message = f"ğŸ”” Trump åœ¨ Truth Social æœ‰æ–°å‹•æ…‹ï¼\n\nğŸ“ é¡å‹: æ–‡å­—\n\nğŸ‡ºğŸ‡¸ åŸæ–‡:\n{processed_content['original_content']}\n\nğŸ‡¹ğŸ‡¼ ä¸­æ–‡ç¿»è­¯:\n{processed_content['translated_content']}"
        
        # å¦‚æœæœ‰åª’é«”ä½†ä¸æ˜¯è¦–é »ï¼Œé™„åŠ åª’é«” URL
        if processed_content['media_urls']:
            message += "\n\nğŸ–¼ï¸ åª’é«”é€£çµ:\n" + "\n".join(processed_content['media_urls'])
        
        # ç™¼é€åˆ° LINE ç¾¤çµ„
        logger.info("æº–å‚™ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„")
        if send_to_line_group(message):
            # ä¿å­˜å·²è™•ç†çš„è²¼æ–‡
            save_post(processed_content['id'], processed_content['original_content'])
            logger.info("è™•ç†å®Œæˆï¼Œè²¼æ–‡å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {str(e)}")
        # åªåœ¨é¦–æ¬¡é‹è¡Œæ™‚ç™¼é€éŒ¯èª¤é€šçŸ¥
        if first_run:
            error_message = f"âŒ é¦–æ¬¡åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {str(e)}"
            send_to_line_group(error_message)
    finally:
        log_shutdown()

if __name__ == "__main__":
    main()
