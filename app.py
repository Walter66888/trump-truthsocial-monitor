import os
import requests
import json
import time
import random
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import hashlib
import sqlite3
from datetime import datetime
import logging

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒè®Šæ•¸é…ç½®
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN")
LINE_GROUP_ID = os.environ.get("LINE_GROUP_ID")

# é©—è­‰ç’°å¢ƒè®Šæ•¸
if not DEEPSEEK_API_KEY:
    logger.error("æœªè¨­ç½® DEEPSEEK_API_KEY ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® DEEPSEEK_API_KEY ç’°å¢ƒè®Šæ•¸")
    
if not LINE_CHANNEL_ACCESS_TOKEN:
    logger.error("æœªè¨­ç½® LINE_CHANNEL_ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® LINE_CHANNEL_ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸")
    
if not LINE_GROUP_ID:
    logger.error("æœªè¨­ç½® LINE_GROUP_ID ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® LINE_GROUP_ID ç’°å¢ƒè®Šæ•¸")

# Truth Social URL
TRUTH_URL = "https://truthsocial.com/@realDonaldTrump"

# è¨­ç½®æ—¥èªŒé ‚éƒ¨
def log_startup():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info("=" * 50)
    logger.info(f"è…³æœ¬å•Ÿå‹•æ™‚é–“: {current_time}")
    logger.info("=" * 50)

# è…³æœ¬çµæŸæ™‚çš„æ—¥èªŒ
def log_shutdown():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info("=" * 50)
    logger.info(f"è…³æœ¬çµæŸæ™‚é–“: {current_time}")
    logger.info("=" * 50)

# åˆå§‹åŒ–æ•¸æ“šåº«
def init_db():
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS posts (
        post_id TEXT PRIMARY KEY,
        content TEXT,
        created_at TIMESTAMP
    )
    ''')
    conn.commit()
    conn.close()
    logger.info("æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")

# æª¢æŸ¥è²¼æ–‡æ˜¯å¦å·²å­˜åœ¨
def is_post_exists(post_id):
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM posts WHERE post_id = ?", (post_id,))
    exists = cursor.fetchone() is not None
    conn.close()
    return exists

# ä¿å­˜æ–°è²¼æ–‡
def save_post(post_id, content):
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO posts (post_id, content, created_at) VALUES (?, ?, ?)",
        (post_id, content, datetime.now())
    )
    conn.commit()
    conn.close()
    logger.info(f"ä¿å­˜è²¼æ–‡ ID: {post_id}")

# é…ç½® Selenium
def setup_selenium():
    logger.info("è®¾ç½® Selenium")
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    
    # ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„ Chromium
    chrome_bin = os.environ.get("CHROME_BIN", "/usr/bin/chromium")
    chrome_options.binary_location = chrome_bin
    
    # ä½¿ç”¨ç³»ç»Ÿå®‰è£…çš„ chromedriver
    try:
        driver = webdriver.Chrome(options=chrome_options)
        logger.info("æˆåŠŸä½¿ç”¨ç³»ç»Ÿ chromedriver å»ºç«‹ driver")
        return driver
    except Exception as e:
        logger.error(f"å»ºç«‹ driver å¤±è´¥: {e}")
        raise
        
# çˆ¬å– Truth Social è²¼æ–‡
def scrape_truth_social():
    logger.info("é–‹å§‹çˆ¬å– Truth Social")
    
    driver = None
    
    try:
        driver = setup_selenium()
        
        # æ·»åŠ éš¨æ©Ÿç”¨æˆ¶ä»£ç†
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.0 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36 Edg/92.0.902.84',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:95.0) Gecko/20100101 Firefox/95.0'
        ]
        driver.execute_cdp_cmd('Network.setUserAgentOverride', {"userAgent": random.choice(user_agents)})
        
        # è¨ªå•é é¢
        driver.get(TRUTH_URL)
        logger.info("å·²è¨ªå• Truth Social é é¢")
        
        # ç­‰å¾…è¼ƒé•·æ™‚é–“ç¢ºä¿é é¢å®Œå…¨åŠ è¼‰
        try:
            logger.info("ç­‰å¾…é é¢å…ƒç´ åŠ è¼‰...")
            # å˜—è©¦å¤šç¨®å¯èƒ½çš„é¸æ“‡å™¨
            selectors = ["article.status-card", "div.status-wrapper", ".truth-social-post", ".post-content", ".timeline-item", "article", "div.post"]
            
            for selector in selectors:
                try:
                    logger.info(f"å˜—è©¦é¸æ“‡å™¨: {selector}")
                    WebDriverWait(driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                    )
                    logger.info(f"é¸æ“‡å™¨ {selector} æˆåŠŸæ‰¾åˆ°å…ƒç´ ")
                    break
                except:
                    logger.info(f"é¸æ“‡å™¨ {selector} æœªæ‰¾åˆ°å…ƒç´ ")
                    continue
            
            # æ·»åŠ æ›´é•·çš„ç­‰å¾…æ™‚é–“
            logger.info("ç­‰å¾…é é¢å®Œå…¨åŠ è¼‰...")
            time.sleep(10)
            
            # æˆªå–å±å¹•æˆªåœ–ä»¥ä¾¿è¨ºæ–·
            driver.save_screenshot("truthsocial_screenshot.png")
            logger.info("å·²ä¿å­˜é é¢æˆªåœ–")
            
            # ç²å–æ‰€æœ‰é é¢å…§å®¹é€²è¡Œåˆ†æ
            page_source = driver.page_source
            
            # å°‡é é¢æºä»£ç¢¼ä¿å­˜åˆ°æ–‡ä»¶ä¸­ä»¥ä¾¿åˆ†æ
            with open("truthsocial_page.html", "w", encoding="utf-8") as f:
                f.write(page_source)
            logger.info("å·²ä¿å­˜é é¢æºä»£ç¢¼")
            
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # å˜—è©¦å°‹æ‰¾ä»»ä½•å¯èƒ½çš„è²¼æ–‡å®¹å™¨å…ƒç´ 
            all_articles = soup.find_all('article')
            all_divs_with_post = soup.find_all('div', class_=lambda x: x and ('post' in x.lower() or 'truth' in x.lower() or 'status' in x.lower()))
            
            logger.info(f"æ‰¾åˆ° {len(all_articles)} å€‹ article å…ƒç´ ")
            logger.info(f"æ‰¾åˆ° {len(all_divs_with_post)} å€‹ç–‘ä¼¼è²¼æ–‡çš„ div å…ƒç´ ")
            
            # å°‹æ‰¾æœ€æ–°çš„è²¼æ–‡
            posts = []
            
            # å˜—è©¦å¤šç¨®å¯èƒ½çš„é¸æ“‡å™¨
            for selector in ['article.status-card', 'div.status-wrapper', '.truth-social-post', '.post-content', '.timeline-item', 'article', 'div.post']:
                posts = soup.select(selector)
                if posts:
                    logger.info(f"ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(posts)} å€‹è²¼æ–‡")
                    break
            
            if not posts and all_articles:
                posts = all_articles
                logger.info(f"ä½¿ç”¨æ‰€æœ‰ article å…ƒç´ ä½œç‚ºå‚™ç”¨")
            
            if not posts and all_divs_with_post:
                posts = all_divs_with_post
                logger.info(f"ä½¿ç”¨å¯èƒ½çš„è²¼æ–‡ div å…ƒç´ ä½œç‚ºå‚™ç”¨")
            
            if not posts:
                logger.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½•å¯èƒ½çš„è²¼æ–‡å…ƒç´ ")
                return None
            
            latest_post = posts[0]
            logger.info("æ‰¾åˆ°æœ€æ–°è²¼æ–‡")
            
            # æå–è²¼æ–‡å…§å®¹ï¼ˆå˜—è©¦å¤šç¨®æ–¹æ³•ï¼‰
            content = None
            
            # æ–¹æ³• 1: ç›´æ¥æ‰¾å†…å®¹å…ƒç´ 
            content_selectors = ['div.status-content', 'div.status-body', '.post-content', '.truth-content', 'p', '.text']
            for selector in content_selectors:
                content_element = latest_post.select_one(selector)
                if content_element:
                    content = content_element.text.strip()
                    logger.info(f"ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ°è²¼æ–‡å…§å®¹")
                    break
            
            # æ–¹æ³• 2: å¦‚æœæ²’æ‰¾åˆ°ç‰¹å®šå†…å®¹å…ƒç´ ï¼Œä½¿ç”¨æ•´å€‹è²¼æ–‡çš„æ–‡æœ¬
            if not content:
                content = latest_post.get_text(separator=' ', strip=True)
                logger.info("ä½¿ç”¨æ•´å€‹è²¼æ–‡çš„æ–‡æœ¬ä½œç‚ºå…§å®¹")
            
            if not content:
                logger.warning("ç„¡æ³•æå–è²¼æ–‡å…§å®¹")
                return None
                
            # ç”Ÿæˆè²¼æ–‡ ID
            post_id = hashlib.md5(content.encode()).hexdigest()
            
            # æª¢æŸ¥æ˜¯å¦æœ‰åª’é«”
            media_urls = []
            
            # å°‹æ‰¾æ‰€æœ‰åœ–ç‰‡å’Œè¦–é »å…ƒç´ 
            for img in latest_post.find_all('img'):
                src = img.get('src') or img.get('data-src')
                if src and not src.endswith(('.svg', '.ico')):
                    if not src.startswith(('http://', 'https://')):
                        src = f"https://truthsocial.com{src}" if src.startswith('/') else f"https://truthsocial.com/{src}"
                    media_urls.append(src)
                    
            for video in latest_post.find_all('video'):
                src = video.get('src') or video.get('data-src')
                if src:
                    if not src.startswith(('http://', 'https://')):
                        src = f"https://truthsocial.com{src}" if src.startswith('/') else f"https://truthsocial.com/{src}"
                    media_urls.append(src)
            
            logger.info(f"è²¼æ–‡ ID: {post_id}, åª’é«”æ•¸é‡: {len(media_urls)}")
            return {
                'id': post_id,
                'content': content,
                'media_urls': media_urls
            }
            
        except Exception as e:
            logger.error(f"è™•ç†é é¢å…§å®¹æ™‚å‡ºéŒ¯: {e}")

# ä½¿ç”¨ DeepSeek API ç¿»è­¯å…§å®¹
def translate_with_deepseek(text):
    logger.info("ä½¿ç”¨ DeepSeek API ç¿»è­¯")
    
    try:
        # ä½¿ç”¨ OpenAI SDK çš„æ–¹å¼ä¾†å‘¼å« DeepSeek API
        from openai import OpenAI
        
        client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ç¿»è­¯ï¼Œè«‹å°‡ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è­¯æˆä¸­æ–‡ã€‚ä¿æŒåŸæ„ï¼Œä½¿èªè¨€æµæš¢è‡ªç„¶ã€‚"},
                {"role": "user", "content": text}
            ],
            temperature=0.3,
            stream=False
        )
        
        translated_text = response.choices[0].message.content
        logger.info("ç¿»è­¯å®Œæˆ")
        return translated_text
        
    except Exception as e:
        logger.error(f"ç¿»è­¯å¤±æ•—: {e}")
        return f"[ç¿»è­¯éŒ¯èª¤] åŸæ–‡: {text}"

# å…§å®¹åˆ†æï¼ˆåˆ¤æ–·æ˜¯æ–‡å­—é‚„æ˜¯è¦–é »ï¼‰
def analyze_content(post):
    if not post:
        return None
        
    # æª¢æŸ¥æ˜¯å¦åŒ…å«è¦–é »
    is_video = any(url.endswith(('.mp4', '.avi', '.mov', '.webm')) for url in post.get('media_urls', []))
    
    # ç¿»è­¯æ–‡æœ¬å…§å®¹
    translated_content = translate_with_deepseek(post['content'])
    
    result = {
        'id': post['id'],
        'original_content': post['content'],
        'translated_content': translated_content,
        'media_urls': post['media_urls'],
        'is_video': is_video
    }
    
    content_type = "å½±ç‰‡" if is_video else "æ–‡å­—"
    logger.info(f"å…§å®¹é¡å‹: {content_type}")
    
    return result

# ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„
def send_to_line_group(message):
    logger.info(f"ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„: {message[:50]}...")
    
    if not LINE_CHANNEL_ACCESS_TOKEN:
        error = "ERROR: LINE_CHANNEL_ACCESS_TOKEN æœªè¨­ç½®"
        logger.error(error)
        return False
        
    if not LINE_GROUP_ID:
        error = "ERROR: LINE_GROUP_ID æœªè¨­ç½®"
        logger.error(error)
        return False
    
    url = 'https://api.line.me/v2/bot/message/push'
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {LINE_CHANNEL_ACCESS_TOKEN}'
    }
    
    data = {
        'to': LINE_GROUP_ID,
        'messages': [
            {
                'type': 'text',
                'text': message
            }
        ]
    }
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        
        # è¼¸å‡ºè©³ç´°å›æ‡‰
        logger.info(f"LINE API éŸ¿æ‡‰ç‹€æ…‹ç¢¼: {response.status_code}")
        logger.info(f"LINE API éŸ¿æ‡‰å…§å®¹: {response.text}")
        
        response.raise_for_status()
        logger.info("LINE æ¶ˆæ¯ç™¼é€æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"LINE æ¶ˆæ¯ç™¼é€å¤±æ•—: {e}")
        return False

# ä¸»æµç¨‹
def main():
    try:
        log_startup()
        
        # ç¬¬ä¸€æ¬¡å•Ÿå‹•æ™‚ç™¼é€é€šçŸ¥ï¼Œç”¨æ–¼ç¢ºèªæ©Ÿå™¨äººæ­£å¸¸å·¥ä½œ
        first_run_file = "first_run_completed.txt"
        first_run = not os.path.exists(first_run_file)
        
        if first_run:
            send_to_line_group("ğŸ¤– Trump ç›£æ§æ©Ÿå™¨äººé¦–æ¬¡å•Ÿå‹•ï¼Œæ­£åœ¨æª¢æŸ¥ Truth Social...")
        
        # åˆå§‹åŒ–æ•¸æ“šåº«
        init_db()
        
        # çˆ¬å–æœ€æ–°è²¼æ–‡
        latest_post = scrape_truth_social()
        
        if not latest_post:
            if first_run:
                send_to_line_group("ğŸ” é¦–æ¬¡çˆ¬å–æ²’æœ‰æ‰¾åˆ°ä»»ä½•è²¼æ–‡ï¼Œå¯èƒ½æ˜¯ç¶²é çµæ§‹è®ŠåŒ–æˆ–è€…çˆ¬èŸ²å•é¡Œã€‚")
            return
            
        # åªåœ¨é¦–æ¬¡é‹è¡Œæ™‚ç™¼é€çˆ¬å–çµæœé€šçŸ¥
        if first_run:
            post_info = f"âœ… é¦–æ¬¡çˆ¬å–æˆåŠŸï¼æ‰¾åˆ°è²¼æ–‡ï¼\n\nID: {latest_post['id']}\n\nå…§å®¹: {latest_post['content'][:100]}...\n\nåª’é«”æ•¸é‡: {len(latest_post['media_urls'])}"
            send_to_line_group(post_info)
            # æ¨™è¨˜é¦–æ¬¡é‹è¡Œå·²å®Œæˆ
            with open(first_run_file, "w") as f:
                f.write("completed")
        
        # æª¢æŸ¥è²¼æ–‡æ˜¯å¦å·²å­˜åœ¨
        if is_post_exists(latest_post['id']):
            logger.info(f"è²¼æ–‡ {latest_post['id']} å·²å­˜åœ¨ï¼Œè·³éè™•ç†")
            return  # éœé»˜è·³éï¼Œä¸ç™¼é€é€šçŸ¥
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå½±ç‰‡è²¼æ–‡
        is_video = any(url.endswith(('.mp4', '.avi', '.mov', '.webm')) for url in latest_post.get('media_urls', []))
        
        if is_video:
            # éœé»˜ç•¥éå½±ç‰‡è²¼æ–‡ï¼Œä½†ä»ç„¶ä¿å­˜åˆ°æ•¸æ“šåº«
            logger.info("æª¢æ¸¬åˆ°å½±ç‰‡è²¼æ–‡ï¼Œç•¥éè™•ç†")
            save_post(latest_post['id'], latest_post['content'])
            return
            
        # åˆ†æä¸¦ç¿»è­¯å…§å®¹ï¼ˆä¸ç™¼é€é€²åº¦é€šçŸ¥ï¼‰
        logger.info("é–‹å§‹åˆ†æä¸¦ç¿»è­¯å…§å®¹")
        processed_content = analyze_content(latest_post)
        
        if not processed_content:
            logger.error("å…§å®¹è™•ç†å¤±æ•—")
            return  # è™•ç†å¤±æ•—ï¼Œéœé»˜è·³é
            
        # æ§‹å»º LINE æ¶ˆæ¯
        message = f"ğŸ”” Trump åœ¨ Truth Social æœ‰æ–°å‹•æ…‹ï¼\n\nğŸ“ é¡å‹: æ–‡å­—\n\nğŸ‡ºğŸ‡¸ åŸæ–‡:\n{processed_content['original_content']}\n\nğŸ‡¹ğŸ‡¼ ä¸­æ–‡ç¿»è­¯:\n{processed_content['translated_content']}"
        
        # å¦‚æœæœ‰åª’é«”ä½†ä¸æ˜¯è¦–é »ï¼Œé™„åŠ åª’é«” URL
        if processed_content['media_urls']:
            message += "\n\nğŸ–¼ï¸ åª’é«”é€£çµ:\n" + "\n".join(processed_content['media_urls'])
        
        # ç™¼é€åˆ° LINE ç¾¤çµ„
        logger.info("æº–å‚™ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„")
        if send_to_line_group(message):
            # ä¿å­˜å·²è™•ç†çš„è²¼æ–‡
            save_post(processed_content['id'], processed_content['original_content'])
            logger.info("è™•ç†å®Œæˆï¼Œè²¼æ–‡å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {str(e)}")
        # åªåœ¨é¦–æ¬¡é‹è¡Œæ™‚ç™¼é€éŒ¯èª¤é€šçŸ¥
        if first_run:
            error_message = f"âŒ é¦–æ¬¡åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {str(e)}"
            send_to_line_group(error_message)
    finally:
        log_shutdown()

if __name__ == "__main__":
    main()
