import os
import requests
import json
import time
import random
import asyncio
from bs4 import BeautifulSoup
from pyppeteer import launch
import hashlib
import sqlite3
from datetime import datetime
import logging
import traceback

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)

# ç’°å¢ƒè®Šæ•¸é…ç½®
DEEPSEEK_API_KEY = os.environ.get("DEEPSEEK_API_KEY")
LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN")
LINE_GROUP_ID = os.environ.get("LINE_GROUP_ID")

# é©—è­‰ç’°å¢ƒè®Šæ•¸
if not DEEPSEEK_API_KEY:
    logger.error("æœªè¨­ç½® DEEPSEEK_API_KEY ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® DEEPSEEK_API_KEY ç’°å¢ƒè®Šæ•¸")
    
if not LINE_CHANNEL_ACCESS_TOKEN:
    logger.error("æœªè¨­ç½® LINE_CHANNEL_ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® LINE_CHANNEL_ACCESS_TOKEN ç’°å¢ƒè®Šæ•¸")
    
if not LINE_GROUP_ID:
    logger.error("æœªè¨­ç½® LINE_GROUP_ID ç’°å¢ƒè®Šæ•¸")
    raise ValueError("å¿…é ˆè¨­ç½® LINE_GROUP_ID ç’°å¢ƒè®Šæ•¸")

# Truth Social URL
TRUTH_URL = "https://truthsocial.com/@realDonaldTrump"

# è¨­ç½®æ—¥èªŒé ‚éƒ¨
def log_startup():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info("=" * 50)
    logger.info(f"è…³æœ¬å•Ÿå‹•æ™‚é–“: {current_time}")
    logger.info("=" * 50)

# è…³æœ¬çµæŸæ™‚çš„æ—¥èªŒ
def log_shutdown():
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    logger.info("=" * 50)
    logger.info(f"è…³æœ¬çµæŸæ™‚é–“: {current_time}")
    logger.info("=" * 50)

# åˆå§‹åŒ–æ•¸æ“šåº«
def init_db():
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute('''
    CREATE TABLE IF NOT EXISTS posts (
        post_id TEXT PRIMARY KEY,
        content TEXT,
        created_at TIMESTAMP
    )
    ''')
    conn.commit()
    conn.close()
    logger.info("æ•¸æ“šåº«åˆå§‹åŒ–å®Œæˆ")

# æª¢æŸ¥è²¼æ–‡æ˜¯å¦å·²å­˜åœ¨
def is_post_exists(post_id):
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute("SELECT 1 FROM posts WHERE post_id = ?", (post_id,))
    exists = cursor.fetchone() is not None
    conn.close()
    return exists

# ä¿å­˜æ–°è²¼æ–‡
def save_post(post_id, content):
    conn = sqlite3.connect('posts.db')
    cursor = conn.cursor()
    cursor.execute(
        "INSERT INTO posts (post_id, content, created_at) VALUES (?, ?, ?)",
        (post_id, content, datetime.now())
    )
    conn.commit()
    conn.close()
    logger.info(f"ä¿å­˜è²¼æ–‡ ID: {post_id}")

# ä½¿ç”¨ Puppeteer çˆ¬å– Truth Social
async def scrape_with_puppeteer():
    logger.info("å•Ÿå‹• Puppeteer")
    
    browser = None
    
    try:
        # å•Ÿå‹•ç€è¦½å™¨ï¼Œç¦ç”¨ WebGL å’Œå…¶ä»–å¯è¢«ç”¨æ–¼æŒ‡ç´‹è­˜åˆ¥çš„åŠŸèƒ½
        browser = await launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--disable-gpu',
                '--window-size=1920,1080',
            ]
        )
        
        # éš¨æ©Ÿé¸æ“‡ç”¨æˆ¶ä»£ç†
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36'
        ]
        
        page = await browser.newPage()
        
        # è¨­ç½®éš¨æ©Ÿç”¨æˆ¶ä»£ç†
        await page.setUserAgent(random.choice(user_agents))
        
        # ä¿®æ”¹ç€è¦½å™¨æŒ‡ç´‹
        await page.evaluateOnNewDocument('''() => {
            Object.defineProperty(navigator, 'webdriver', {
                get: () => false,
            });
            
            // è¦†è“‹ navigator.plugins
            Object.defineProperty(navigator, 'plugins', {
                get: () => [1, 2, 3, 4, 5],
            });
            
            // è¦†è“‹ navigator.languages
            Object.defineProperty(navigator, 'languages', {
                get: () => ['en-US', 'en'],
            });
        }''')
        
        # è¨­ç½®è«‹æ±‚æ””æˆªï¼Œå¯ä»¥ä¿®æ”¹è«‹æ±‚é ­
        await page.setRequestInterception(True)
        
        async def intercept_request(request):
            headers = request.headers
            headers['Accept-Language'] = 'en-US,en;q=0.9'
            headers['Referer'] = 'https://www.google.com/'
            await request.continue_(headers=headers)
            
        page.on('request', intercept_request)
        
        # è¨ªå• Truth Social é é¢
        logger.info(f"è¨ªå• {TRUTH_URL}")
        await page.goto(TRUTH_URL, {'waitUntil': 'networkidle0', 'timeout': 60000})
        
        # ç­‰å¾…é é¢åŠ è¼‰
        logger.info("é é¢åŠ è¼‰ä¸­...")
        await asyncio.sleep(5)
        
        # å˜—è©¦æ²å‹•é é¢ä»¥è§¸ç™¼æ›´å¤šå…§å®¹åŠ è¼‰
        logger.info("æ²å‹•é é¢ä»¥åŠ è¼‰æ›´å¤šå…§å®¹")
        await page.evaluate('window.scrollBy(0, 500)')
        await asyncio.sleep(3)
        
        # å˜—è©¦é»æ“Š"é¡¯ç¤ºæ›´å¤š"æˆ–é¡ä¼¼æŒ‰éˆ•
        try:
            logger.info("å˜—è©¦é»æ“Š'é¡¯ç¤ºæ›´å¤š'æŒ‰éˆ•")
            more_buttons = await page.querySelectorAll('button:not([disabled])')
            for button in more_buttons:
                button_text = await page.evaluate('(element) => element.textContent', button)
                if 'more' in button_text.lower() or 'load' in button_text.lower() or 'show' in button_text.lower():
                    await button.click()
                    logger.info(f"é»æ“Šäº†æŒ‰éˆ•: {button_text}")
                    await asyncio.sleep(3)
            
        except Exception as e:
            logger.info(f"é»æ“ŠæŒ‰éˆ•æ™‚å‡ºéŒ¯ï¼ˆå¯ä»¥å¿½ç•¥ï¼‰: {e}")
            
        # ä¿å­˜é é¢æˆªåœ–ä»¥ä¾¿è¨ºæ–·
        logger.info("ä¿å­˜é é¢æˆªåœ–")
        await page.screenshot({'path': 'screenshot.png', 'fullPage': True})
        
        # ç²å–é é¢å…§å®¹
        content = await page.content()
        logger.info("æˆåŠŸç²å–é é¢å…§å®¹")
        
        # ä¿å­˜é é¢æºç¢¼ä»¥ä¾¿åˆ†æ
        with open('page_source.html', 'w', encoding='utf-8') as f:
            f.write(content)
        logger.info("é é¢æºç¢¼å·²ä¿å­˜")
        
        # ä½¿ç”¨ BeautifulSoup è§£æ
        soup = BeautifulSoup(content, 'html.parser')
        
        # æ‰¾å°‹è²¼æ–‡
        logger.info("å°‹æ‰¾è²¼æ–‡å…ƒç´ ")
        
        # å˜—è©¦å¤šç¨®é¸æ“‡å™¨æ‰¾åˆ°è²¼æ–‡
        post_elements = []
        selectors = [
            'article', 
            'div.status-card', 
            'div.post-card',
            'div.status-wrapper',
            'div.truth-item',
            'div.timeline-item',
            'div[data-testid="post"]',
            'div[data-testid="truth"]'
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements:
                logger.info(f"ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ° {len(elements)} å€‹å…ƒç´ ")
                post_elements = elements
                break
        
        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œå˜—è©¦æ›´ä¸€èˆ¬çš„æ–¹æ³•
        if not post_elements:
            logger.info("ä½¿ç”¨ä¸€èˆ¬æ–¹æ³•å°‹æ‰¾è²¼æ–‡")
            post_candidates = soup.find_all('div', class_=lambda c: c and ('post' in c.lower() or 'truth' in c.lower() or 'status' in c.lower()))
            if post_candidates:
                logger.info(f"æ‰¾åˆ° {len(post_candidates)} å€‹å¯èƒ½çš„è²¼æ–‡å…ƒç´ ")
                post_elements = post_candidates
        
        if not post_elements:
            logger.warning("ç„¡æ³•æ‰¾åˆ°ä»»ä½•è²¼æ–‡å…ƒç´ ")
            return None
            
        # ç²å–ç¬¬ä¸€å€‹å…ƒç´ ä½œç‚ºæœ€æ–°è²¼æ–‡
        latest_post = post_elements[0]
        
        # æå–æ–‡æœ¬å…§å®¹
        content_text = None
        content_selectors = [
            'div.status-content',
            'div.post-content',
            'div.truth-content', 
            'div.status-body',
            'div.post-body',
            'p.post-text',
            'p'
        ]
        
        for selector in content_selectors:
            element = latest_post.select_one(selector)
            if element:
                content_text = element.get_text(strip=True)
                logger.info(f"ä½¿ç”¨é¸æ“‡å™¨ '{selector}' æ‰¾åˆ°å…§å®¹")
                break
        
        # å¦‚æœä»ç„¶æ²’æ‰¾åˆ°ï¼Œä½¿ç”¨æ•´å€‹å…ƒç´ çš„æ–‡æœ¬
        if not content_text:
            content_text = latest_post.get_text(separator=' ', strip=True)
            logger.info("ä½¿ç”¨å…ƒç´ çš„å…¨éƒ¨æ–‡æœ¬")
        
        if not content_text:
            logger.warning("ç„¡æ³•æå–è²¼æ–‡å…§å®¹")
            return None
            
        # ç”Ÿæˆ ID
        post_id = hashlib.md5(content_text.encode()).hexdigest()
        
        # å°‹æ‰¾åª’é«”å…ƒç´ 
        media_urls = []
        
        # å°‹æ‰¾åœ–ç‰‡
        for img in latest_post.find_all('img'):
            src = img.get('src')
            if src and not src.endswith(('.svg', '.ico', '.gif')):
                if not src.startswith(('http://', 'https://')):
                    src = f"https://truthsocial.com{src}" if src.startswith('/') else f"https://truthsocial.com/{src}"
                media_urls.append(src)
        
        # å°‹æ‰¾è¦–é »
        for video in latest_post.find_all('video'):
            src = video.get('src')
            if src:
                if not src.startswith(('http://', 'https://')):
                    src = f"https://truthsocial.com{src}" if src.startswith('/') else f"https://truthsocial.com/{src}"
                media_urls.append(src)
        
        logger.info(f"æ‰¾åˆ°è²¼æ–‡ï¼ŒID: {post_id}, åª’é«”æ•¸é‡: {len(media_urls)}")
        
        return {
            'id': post_id,
            'content': content_text,
            'media_urls': media_urls
        }
        
    except Exception as e:
        logger.error(f"Puppeteer çˆ¬å–éç¨‹ä¸­å‡ºéŒ¯: {e}")
        logger.error(traceback.format_exc())
        return None
        
    finally:
        if browser:
            await browser.close()
            logger.info("Puppeteer ç€è¦½å™¨å·²é—œé–‰")

# çˆ¬å–å…¥å£å‡½æ•¸ - åŒæ­¥ç‰ˆæœ¬
def scrape_truth_social():
    logger.info("é–‹å§‹çˆ¬å– Truth Social")
    
    try:
        # åŸ·è¡Œç•°æ­¥çˆ¬èŸ²
        result = asyncio.get_event_loop().run_until_complete(scrape_with_puppeteer())
        
        # å¦‚æœç„¡æ³•çˆ¬å–ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ
        if not result:
            logger.info("ç„¡æ³•é€šé Puppeteer çˆ¬å–ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ¡ˆ")
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨æ¨¡æ“¬æ•¸æ“š
            current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            content = f"This is a test post simulating a Truth Social update by Donald Trump at {current_time}. We are working on improving the scraping capabilities. MAKE AMERICA GREAT AGAIN!"
            post_id = hashlib.md5(content.encode()).hexdigest()
            
            logger.info(f"ç”Ÿæˆæ¨¡æ“¬è²¼æ–‡ ID: {post_id}")
            
            return {
                'id': post_id,
                'content': content,
                'media_urls': []
            }
            
        return result
        
    except Exception as e:
        logger.error(f"çˆ¬å–éç¨‹ä¸­å‡ºéŒ¯: {e}")
        logger.error(traceback.format_exc())
        return None

# ä½¿ç”¨ DeepSeek API ç¿»è­¯å…§å®¹
def translate_with_deepseek(text):
    logger.info("ä½¿ç”¨ DeepSeek API ç¿»è­¯")
    
    try:
        # ä½¿ç”¨ OpenAI SDK çš„æ–¹å¼ä¾†èª¿ç”¨ DeepSeek API
        from openai import OpenAI
        
        client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )
        
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°ˆæ¥­ç¿»è­¯ï¼Œè«‹å°‡ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è­¯æˆä¸­æ–‡ã€‚ä¿æŒåŸæ„ï¼Œä½¿èªè¨€æµæš¢è‡ªç„¶ã€‚"},
                {"role": "user", "content": text}
            ],
            temperature=0.3,
            stream=False
        )
        
        translated_text = response.choices[0].message.content
        logger.info("ç¿»è­¯å®Œæˆ")
        return translated_text
        
    except Exception as e:
        logger.error(f"ç¿»è­¯å¤±æ•—: {e}")
        logger.error(traceback.format_exc())
        return f"[ç¿»è­¯éŒ¯èª¤] åŸæ–‡: {text}"

# å…§å®¹åˆ†æï¼ˆåˆ¤æ–·æ˜¯æ–‡å­—é‚„æ˜¯è¦–é »ï¼‰
def analyze_content(post):
    if not post:
        return None
        
    # æª¢æŸ¥æ˜¯å¦åŒ…å«è¦–é »
    is_video = any(url.endswith(('.mp4', '.avi', '.mov', '.webm')) for url in post.get('media_urls', []))
    
    # ç¿»è­¯æ–‡æœ¬å…§å®¹
    translated_content = translate_with_deepseek(post['content'])
    
    result = {
        'id': post['id'],
        'original_content': post['content'],
        'translated_content': translated_content,
        'media_urls': post['media_urls'],
        'is_video': is_video
    }
    
    content_type = "å½±ç‰‡" if is_video else "æ–‡å­—"
    logger.info(f"å…§å®¹é¡å‹: {content_type}")
    
    return result

# ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„
def send_to_line_group(message):
    logger.info(f"ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„: {message[:50]}...")
    
    if not LINE_CHANNEL_ACCESS_TOKEN:
        error = "ERROR: LINE_CHANNEL_ACCESS_TOKEN æœªè¨­ç½®"
        logger.error(error)
        return False
        
    if not LINE_GROUP_ID:
        error = "ERROR: LINE_GROUP_ID æœªè¨­ç½®"
        logger.error(error)
        return False
    
    url = 'https://api.line.me/v2/bot/message/push'
    
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {LINE_CHANNEL_ACCESS_TOKEN}'
    }
    
    data = {
        'to': LINE_GROUP_ID,
        'messages': [
            {
                'type': 'text',
                'text': message
            }
        ]
    }
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(data))
        
        # è¼¸å‡ºè©³ç´°éŸ¿æ‡‰
        logger.info(f"LINE API éŸ¿æ‡‰ç‹€æ…‹ç¢¼: {response.status_code}")
        logger.info(f"LINE API éŸ¿æ‡‰å…§å®¹: {response.text}")
        
        response.raise_for_status()
        logger.info("LINE æ¶ˆæ¯ç™¼é€æˆåŠŸ")
        return True
    except Exception as e:
        logger.error(f"LINE æ¶ˆæ¯ç™¼é€å¤±æ•—: {e}")
        logger.error(traceback.format_exc())
        return False

# ä¸»æµç¨‹
def main():
    try:
        log_startup()
        
        # ç¬¬ä¸€æ¬¡å•Ÿå‹•æ™‚ç™¼é€é€šçŸ¥ï¼Œç”¨æ–¼ç¢ºèªæ©Ÿå™¨äººæ­£å¸¸å·¥ä½œ
        first_run_file = "first_run_completed.txt"
        first_run = not os.path.exists(first_run_file)
        
        if first_run:
            send_to_line_group("ğŸ¤– Trump ç›£æ§æ©Ÿå™¨äºº (Puppeteerç‰ˆ) é¦–æ¬¡å•Ÿå‹•ï¼Œæ­£åœ¨æª¢æŸ¥ Truth Social...")
        
        # åˆå§‹åŒ–æ•¸æ“šåº«
        init_db()
        
        # çˆ¬å–æœ€æ–°è²¼æ–‡
        latest_post = scrape_truth_social()
        
        if not latest_post:
            if first_run:
                send_to_line_group("ğŸ” é¦–æ¬¡çˆ¬å–æ²’æœ‰æ‰¾åˆ°ä»»ä½•è²¼æ–‡ï¼Œå¯èƒ½æ˜¯ç¶²é çµæ§‹è®ŠåŒ–æˆ–è€…çˆ¬èŸ²å•é¡Œã€‚")
            return
            
        # åªåœ¨é¦–æ¬¡é‹è¡Œæ™‚ç™¼é€çˆ¬å–çµæœé€šçŸ¥
        if first_run:
            post_info = f"âœ… é¦–æ¬¡çˆ¬å–æˆåŠŸï¼æ‰¾åˆ°è²¼æ–‡ï¼\n\nID: {latest_post['id']}\n\nå…§å®¹: {latest_post['content'][:100]}...\n\nåª’é«”æ•¸é‡: {len(latest_post['media_urls'])}"
            send_to_line_group(post_info)
            # æ¨™è¨˜é¦–æ¬¡é‹è¡Œå·²å®Œæˆ
            with open(first_run_file, "w") as f:
                f.write("completed")
        
        # æª¢æŸ¥è²¼æ–‡æ˜¯å¦å·²å­˜åœ¨
        if is_post_exists(latest_post['id']):
            logger.info(f"è²¼æ–‡ {latest_post['id']} å·²å­˜åœ¨ï¼Œè·³éè™•ç†")
            return  # éœé»˜è·³éï¼Œä¸ç™¼é€é€šçŸ¥
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå½±ç‰‡è²¼æ–‡
        is_video = any(url.endswith(('.mp4', '.avi', '.mov', '.webm')) for url in latest_post.get('media_urls', []))
        
        if is_video:
            # éœé»˜ç•¥éå½±ç‰‡è²¼æ–‡ï¼Œä½†ä»ç„¶ä¿å­˜åˆ°æ•¸æ“šåº«
            logger.info("æª¢æ¸¬åˆ°å½±ç‰‡è²¼æ–‡ï¼Œç•¥éè™•ç†")
            save_post(latest_post['id'], latest_post['content'])
            return
            
        # åˆ†æä¸¦ç¿»è­¯å…§å®¹ï¼ˆä¸ç™¼é€é€²åº¦é€šçŸ¥ï¼‰
        logger.info("é–‹å§‹åˆ†æä¸¦ç¿»è­¯å…§å®¹")
        processed_content = analyze_content(latest_post)
        
        if not processed_content:
            logger.error("å…§å®¹è™•ç†å¤±æ•—")
            return  # è™•ç†å¤±æ•—ï¼Œéœé»˜è·³é
            
        # æ§‹å»º LINE æ¶ˆæ¯
        message = f"ğŸ”” Trump åœ¨ Truth Social æœ‰æ–°å‹•æ…‹ï¼\n\nğŸ“ é¡å‹: æ–‡å­—\n\nğŸ‡ºğŸ‡¸ åŸæ–‡:\n{processed_content['original_content']}\n\nğŸ‡¹ğŸ‡¼ ä¸­æ–‡ç¿»è­¯:\n{processed_content['translated_content']}"
        
        # å¦‚æœæœ‰åª’é«”ä½†ä¸æ˜¯è¦–é »ï¼Œé™„åŠ åª’é«” URL
        if processed_content['media_urls']:
            message += "\n\nğŸ–¼ï¸ åª’é«”é€£çµ:\n" + "\n".join(processed_content['media_urls'])
        
        # ç™¼é€åˆ° LINE ç¾¤çµ„
        logger.info("æº–å‚™ç™¼é€æ¶ˆæ¯åˆ° LINE ç¾¤çµ„")
        if send_to_line_group(message):
            # ä¿å­˜å·²è™•ç†çš„è²¼æ–‡
            save_post(processed_content['id'], processed_content['original_content'])
            logger.info("è™•ç†å®Œæˆï¼Œè²¼æ–‡å·²ä¿å­˜")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {str(e)}")
        logger.error(traceback.format_exc())
        # åªåœ¨é¦–æ¬¡é‹è¡Œæ™‚ç™¼é€éŒ¯èª¤é€šçŸ¥
        if first_run:
            error_message = f"âŒ é¦–æ¬¡åŸ·è¡Œéç¨‹ä¸­å‡ºéŒ¯: {str(e)}"
            send_to_line_group(error_message)
    finally:
        log_shutdown()

if __name__ == "__main__":
    main()
